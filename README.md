# Advanced command-r-08-2024

## Setup
1. `pip install -r requiments.txt`
2. Host command-r-08-2024 using llama.cpp/vllm or something at http://localhost:8081. This code uses the completions api.
3. `python chat.py`

## Thanks to
https://huggingface.co/mattshumer/Reflection-Llama-3.1-70B