# Advanced gemma2(changed from advanced command-r-082024)

## Setup
1. `pip install -r requiments.txt`
2. Host gemma2 using llama.cpp/vllm or something at http://localhost:8081. This code uses the completions api.
3. `python chat.py`

## Thanks to
https://huggingface.co/mattshumer/Reflection-Llama-3.1-70B